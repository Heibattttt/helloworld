newdat <- read.csv('kc_house_data.csv')
colnames(newdat)

library(psych)   # for describe() if you used it previously

describe(newdat)
summary(newdat)
################################################################################
# Step 1: Histograms (log-transformed)
################################################################################
numeric_cols <- c("price","bedrooms","bathrooms","sqft_living","sqft_lot",
                  "floors","waterfront","view","condition","grade",
                  "sqft_above","sqft_basement","yr_built","yr_renovated",
                  "lat","long","sqft_living15","sqft_lot15")

# Create log versions using log1p (safe for zeros)
# Variables that make sense to log
loggable_cols <- c("price","sqft_living","sqft_lot","sqft_above",
                   "sqft_basement","sqft_living15","sqft_lot15")

for (col in loggable_cols) {
  newdat[[paste0(col, "_log")]] <- log1p(newdat[[col]])
}

# Plot histograms of the log-transformed variables
log_cols <- paste0(numeric_cols, "_log")

par(mfrow=c(3,6))  # 18 numeric variables
for (col in log_cols) {
  hist(newdat[[col]], main=col, xlab=col, col="skyblue", border="white")
}
par(mfrow=c(1,1))

pdf("histograms_log.pdf", width=14, height=8)
par(mfrow=c(3,6))
for (col in log_cols) {
  hist(newdat[[col]], main=col, xlab=col, col="skyblue", border="white")
}
par(mfrow=c(1,1))
dev.off()

# Correlations on logged variables (use price_log)
numeric_log_cols_for_cor <- log_cols
correlations <- cor(newdat[, numeric_log_cols_for_cor], use = "complete.obs")

# View the correlation of all variables with price_log
cor_with_price_log <- correlations["price_log", ]
print(cor_with_price_log)

# Sort correlations by strength (descending)
sort(cor_with_price_log, decreasing = TRUE)
barplot(cor_with_price_log,
        main="Correlation of Each Log-Transformed Variable with price_log",
        ylab="Correlation Coefficient",
        xlab="Variables",
        col="skyblue",
        las=2,
        cex.names=0.8)

cor_matrix <- correlations
cor_values <- cor_matrix[lower.tri(cor_matrix)]
hist(cor_values,
     breaks=20,
     main="Histogram of Pairwise Correlations (logged vars)",
     xlab="Correlation Coefficient",
     col="skyblue", border="white")

################################################################################
# Step 2: Data preparation (response already logged)
################################################################################
# price_log: use natural log (not log1p here, to match your earlier modelling choice)
newdat$price_log <- log(newdat$price)

# Scale predictors and convert to numeric vectors (use original scale variables, scaled)
newdat$sqft_living_scaled   <- as.vector(scale(newdat$sqft_living))
newdat$grade_scaled         <- as.vector(scale(newdat$grade))
newdat$sqft_above_scaled    <- as.vector(scale(newdat$sqft_above))
newdat$sqft_living15_scaled <- as.vector(scale(newdat$sqft_living15))
newdat$bathrooms_scaled     <- as.vector(scale(newdat$bathrooms))

################################################################################
# Step 3: Model 1 (Simple: sqft_living + grade)
################################################################################
# Remove rows with missing values in the relevant columns
dat1 <- na.omit(newdat[, c("price_log", "sqft_living_scaled", "grade_scaled")])

data_jags_1 <- list(
  y  = dat1$price_log,
  x1 = dat1$sqft_living_scaled,
  x2 = dat1$grade_scaled,
  N  = nrow(dat1)
)

model_string_1 <- "
model {
  for (i in 1:N) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] <- beta0 + beta1*x1[i] + beta2*x2[i]
  }
  beta0 ~ dnorm(0, 0.0001)
  beta1 ~ dnorm(0, 0.0001)
  beta2 ~ dnorm(0, 0.0001)
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1 / sqrt(tau)
}
"

jags_1 <- jags.model(textConnection(model_string_1), data = data_jags_1, n.chains = 3, n.adapt = 1000)
update(jags_1, 1000)

samples_1 <- coda.samples(jags_1,
                          variable.names = c("beta0","beta1","beta2","sigma"),
                          n.iter = 5000)

posterior_matrix_1 <- as.matrix(samples_1)
posterior_summary_1 <- data.frame(
  Parameter = colnames(posterior_matrix_1),
  Mean = apply(posterior_matrix_1, 2, mean),
  SD   = apply(posterior_matrix_1, 2, sd),
  `2.5%` = apply(posterior_matrix_1, 2, quantile, 0.025),
  `97.5%` = apply(posterior_matrix_1, 2, quantile, 0.975)
)
print(posterior_summary_1, digits=5)

# Diagnostics
plot(samples_1)
gelman.diag(samples_1)
effectiveSize(samples_1)

# Posterior predictive check (LOG scale)
set.seed(123)
post_idx_1 <- sample(1:nrow(posterior_matrix_1), 200)
y_rep_1_log <- matrix(NA, nrow=nrow(dat1), ncol=length(post_idx_1))
for (k in seq_along(post_idx_1)) {
  theta <- posterior_matrix_1[post_idx_1[k],]
  mu_k <- theta["beta0"] + theta["beta1"]*dat1$sqft_living_scaled + theta["beta2"]*dat1$grade_scaled
  # simulate on log scale (no exp)
  y_rep_1_log[,k] <- rnorm(nrow(dat1), mean = mu_k, sd = theta["sigma"])
}

par(mfrow=c(1,2))
hist(dat1$price_log, main="Observed price_log (Model 1 data)", col="lightblue", xlab="price_log")
hist(as.vector(y_rep_1_log), main="Posterior Predictive (Model 1) on log scale", col="lightgreen", xlab="price_log")
par(mfrow=c(1,1))

################################################################################
# Step 4: Model 2 (Extended: sqft_living + grade + sqft_above + sqft_living15 + bathrooms)
################################################################################
dat2 <- na.omit(newdat[, c("price_log", "sqft_living_scaled", "grade_scaled",
                           "sqft_above_scaled","sqft_living15_scaled","bathrooms_scaled")])

data_jags_2 <- list(
  y  = dat2$price_log,
  x1 = dat2$sqft_living_scaled,
  x2 = dat2$grade_scaled,
  x3 = dat2$sqft_above_scaled,
  x4 = dat2$sqft_living15_scaled,
  x5 = dat2$bathrooms_scaled,
  N  = nrow(dat2)
)

model_string_2 <- "
model {
  for (i in 1:N) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] <- beta0 + beta1*x1[i] + beta2*x2[i] + beta3*x3[i] + beta4*x4[i] + beta5*x5[i]
  }
  beta0 ~ dnorm(0, 0.0001)
  beta1 ~ dnorm(0, 0.0001)
  beta2 ~ dnorm(0, 0.0001)
  beta3 ~ dnorm(0, 0.0001)
  beta4 ~ dnorm(0, 0.0001)
  beta5 ~ dnorm(0, 0.0001)
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1 / sqrt(tau)
}
"

jags_2 <- jags.model(textConnection(model_string_2), data = data_jags_2, n.chains = 3, n.adapt = 1000)
update(jags_2, 1000)

samples_2 <- coda.samples(jags_2,
                          variable.names = c("beta0","beta1","beta2","beta3","beta4","beta5","sigma"),
                          n.iter = 5000)

posterior_matrix_2 <- as.matrix(samples_2)
posterior_summary_2 <- data.frame(
  Parameter = colnames(posterior_matrix_2),
  Mean = apply(posterior_matrix_2, 2, mean),
  SD   = apply(posterior_matrix_2, 2, sd),
  `2.5%` = apply(posterior_matrix_2, 2, quantile, 0.025),
  `97.5%` = apply(posterior_matrix_2, 2, quantile, 0.975)
)
print(posterior_summary_2, digits=5)

# Diagnostics
plot(samples_2)
gelman.diag(samples_2)
effectiveSize(samples_2)

# Posterior predictive check (LOG scale)
set.seed(123)
post_idx_2 <- sample(1:nrow(posterior_matrix_2), 200)
y_rep_2_log <- matrix(NA, nrow=nrow(dat2), ncol=length(post_idx_2))
for (k in seq_along(post_idx_2)) {
  theta <- posterior_matrix_2[post_idx_2[k],]
  mu_k <- theta["beta0"] + theta["beta1"]*dat2$sqft_living_scaled +
    theta["beta2"]*dat2$grade_scaled + theta["beta3"]*dat2$sqft_above_scaled +
    theta["beta4"]*dat2$sqft_living15_scaled + theta["beta5"]*dat2$bathrooms_scaled
  # simulate on log scale (no exp)
  y_rep_2_log[,k] <- rnorm(nrow(dat2), mean = mu_k, sd = theta["sigma"])
}

par(mfrow=c(1,2))
hist(dat2$price_log, main="Observed price_log (Model 2 data)", col="lightblue", xlab="price_log")
hist(as.vector(y_rep_2_log), main="Posterior Predictive (Model 2) on log scale", col="lightgreen", xlab="price_log")
par(mfrow=c(1,1))

################################################################################
# Step 5: Model comparison with DIC (and OLS on log scale)
################################################################################

# Frequentist OLS on log scale for fair comparison
ols_two_log <- lm(price_log ~ scale(sqft_living) + scale(grade), data = newdat)
summary(ols_two_log)

ols_full_log <- lm(price_log ~ scale(sqft_living) + scale(grade) +
                     scale(sqft_above) + scale(sqft_living15) + scale(bathrooms),
                   data = newdat)
summary(ols_full_log)

dic_1 <- dic.samples(jags_1, n.iter=5000, type="pD")
dic_2 <- dic.samples(jags_2, n.iter=5000, type="pD")

dic_table <- data.frame(
  Model = c("Model 1 (2 predictors)","Model 2 (5 predictors)"),
  MeanDeviance = c(mean(dic_1$deviance), mean(dic_2$deviance)),
  pD = c(mean(dic_1$penalty), mean(dic_2$penalty)),
  DIC = c(mean(dic_1$deviance)+mean(dic_1$penalty),
          mean(dic_2$deviance)+mean(dic_2$penalty))
)
print(dic_table, digits=4)
